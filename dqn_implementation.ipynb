{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006fa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b34486",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_in_matrix = np.array([[ 1.21681603-7.02960064e-01j,  0.20290016+8.51344161e-02j],\n",
    " [ 0.2235912 -1.31483340e+00j, -0.05658839-4.33710249e-01j],\n",
    " [ 0.5285306 +3.40563893e-01j,  0.08218581+4.79234245e-02j],\n",
    " [ 1.23399194-6.24933708e-01j,  0.03170967-1.04554245e-01j],\n",
    " [ 0.45417008+3.60881755e-01j,  0.38861016-2.02170038e-01j],\n",
    " [-1.16104127+6.48951958e-01j, -0.3119777 +3.26512203e-01j],\n",
    " [ 1.03986114-1.55374893e+00j,  0.17419368-1.04203219e-01j],\n",
    " [-0.4700263 -2.77507106e-01j, -0.22915376-4.03178363e-01j],\n",
    " [ 1.53279626+1.00990741e+00j,  0.25040745+3.69172931e-01j],\n",
    " [ 1.07487597-1.43220924e+00j,  0.08041188-3.49898300e-01j],\n",
    " [ 1.50758164+1.03282587e+00j,  0.3850454 -5.88166124e-02j],\n",
    " [-1.52938941-1.05562909e+00j, -0.03184431+7.69364299e-02j],\n",
    " [-0.36085301+5.09021498e-01j, -0.31843067-1.95382769e-01j],\n",
    " [-1.11420351+1.50428028e+00j,  0.13378496+3.53150792e-01j],\n",
    " [-1.09057229+1.52395699e+00j, -0.2528942 +2.20647758e-01j],\n",
    " [-0.21500919+1.35401978e+00j,  0.03030719+4.17020990e-01j],\n",
    " [ 0.46934761+3.57674088e-01j,  0.11145662-2.30844481e-04j],\n",
    " [-1.23806194+6.63268501e-01j,  0.00246224+1.20520694e-01j],\n",
    " [-0.20838795+1.38006558e+00j, -0.2377867 +1.76439435e-01j],\n",
    " [ 0.64261116+1.21850647e+00j,  0.25403243+3.08359474e-01j],\n",
    " [ 1.37763579+2.29178423e-01j,  0.09108907-4.21485193e-03j],\n",
    " [ 0.32265284-5.39511793e-01j,  0.42013826-2.24724449e-02j],\n",
    " [-1.46965378-1.04200389e+00j, -0.46156852-1.72363502e-01j],\n",
    " [ 1.25360397-6.97182713e-01j,  0.00949433+3.28802536e-02j],\n",
    " [ 1.07912537-1.45890826e+00j,  0.12895274-4.54789212e-01j],\n",
    " [ 1.39659123+2.08814903e-01j,  0.14005045-5.49605246e-02j],\n",
    " [ 0.44667237+3.99538234e-01j,  0.29308755-2.01128662e-01j],\n",
    " [-1.04421915+1.46756230e+00j, -0.05116305+4.57164169e-01j],\n",
    " [-0.71250941-1.20723242e+00j, -0.10739142-1.26451936e-01j],\n",
    " [-1.01686533+1.51593027e+00j, -0.2680225 +1.17703562e-01j],\n",
    " [ 1.19431051-6.89583450e-01j,  0.14874273+2.33872954e-01j],\n",
    " [-0.52344401-3.18370466e-01j,  0.08560379-4.30060118e-01j],\n",
    " [-1.22056464+6.05588279e-01j, -0.18583521+3.43931310e-01j],\n",
    " [-1.38159909-2.13259049e-01j, -0.17389099-2.47611808e-02j],\n",
    " [-0.47844562-3.86338506e-01j, -0.26649244+1.47880498e-01j],\n",
    " [ 0.36372078-4.82751769e-01j,  0.01016099-2.21795420e-01j],\n",
    " [ 1.26919428-6.81881520e-01j,  0.05352733+9.51467528e-02j],\n",
    " [ 1.04288522-1.44964954e+00j,  0.20702644-5.81127962e-01j],\n",
    " [ 0.71130349+1.19437463e+00j,  0.03047068+1.94399719e-01j],\n",
    " [ 0.13093633-1.35741651e+00j,  0.30676774-1.72944506e-01j],\n",
    " [-1.00833328+1.56612443e+00j, -0.4523246 -4.22056858e-02j],\n",
    " [ 1.45708835+1.01982812e+00j,  0.4437225 +4.40612630e-01j],\n",
    " [-0.34030047+5.92493172e-01j, -0.10329357-1.85283713e-01j],\n",
    " [ 1.50611151+9.95047034e-01j,  0.33056037+4.26109549e-01j],\n",
    " [-0.67437927-1.14246170e+00j, -0.00527618-3.59072838e-01j],\n",
    " [ 0.70844364+1.17683046e+00j, -0.10490104+2.45636232e-01j],\n",
    " [ 1.03820004-1.49662575e+00j,  0.29510364-1.48281668e-01j],\n",
    " [-0.37352395+5.34386342e-01j, -0.01968922-3.23346766e-01j],\n",
    " [-1.0550695 +1.48881956e+00j, -0.09336574+5.15195422e-01j],\n",
    " [ 0.31397423-5.16832459e-01j,  0.02311209-4.11381561e-02j],\n",
    " [-1.19668745+6.82103183e-01j, -0.12371971+4.52976747e-03j],\n",
    " [ 1.38337029+1.76242809e-01j, -0.04023529+2.43790461e-01j],\n",
    " [ 0.32056973-4.57486299e-01j,  0.3459744 -3.22464611e-01j],\n",
    " [ 0.70401393+1.24783328e+00j, -0.24725569+1.61987403e-01j],\n",
    " [ 1.44117128+1.05000283e+00j,  0.69012021+8.56043840e-02j],\n",
    " [-1.3400167 -1.75148258e-01j, -0.28323437+1.25570544e-01j],\n",
    " [ 0.60793143+1.21488745e+00j,  0.09545648-8.20030088e-02j],\n",
    " [-1.19734119+6.14759235e-01j,  0.04018809+5.12501555e-01j],\n",
    " [ 0.17995109-1.34864009e+00j, -0.21468263-3.49153820e-01j],\n",
    " [-1.20512226+6.08817689e-01j,  0.08248081+1.55440252e-01j],\n",
    " [-0.64981116-1.19473890e+00j, -0.43251185-7.32168443e-02j],\n",
    " [ 0.65932877+1.22265736e+00j,  0.14257238-5.19392519e-02j],\n",
    " [ 0.48254449+3.46603030e-01j,  0.15359745+2.20154855e-01j],\n",
    " [-1.39792344-2.37321784e-01j,  0.09963052-4.96259147e-02j],\n",
    " [-1.44642274-1.08285828e+00j, -0.63542955+1.26849922e-01j],\n",
    " [ 1.05440311-1.55892701e+00j,  0.19598803-2.96609568e-01j],\n",
    " [-0.60810839-1.15341335e+00j, -0.3027328 -3.14293408e-01j],\n",
    " [ 1.50185125+1.03451195e+00j,  0.25321588+1.77082638e-02j],\n",
    " [-0.506899  -3.15400859e-01j,  0.04545501+6.51939064e-02j],\n",
    " [ 0.61978727+1.22342055e+00j,  0.0357915 -1.07503162e-01j],\n",
    " [-1.06992138+1.51112810e+00j,  0.09274173+4.66220284e-01j],\n",
    " [ 0.34131198-5.51011155e-01j, -0.05332996+7.85088559e-02j],\n",
    " [-1.52888039-1.04163661e+00j, -0.08529785-2.33680097e-01j],\n",
    " [-0.18841921+1.36681245e+00j, -0.34358777+7.23036797e-02j],\n",
    " [ 0.51553   +3.23183747e-01j,  0.28589322+4.01824967e-01j],\n",
    " [ 1.03960553-1.47258931e+00j, -0.01739149-4.83550579e-01j],\n",
    " [-1.0436919 +1.50155827e+00j,  0.13551446+1.59433884e-01j],\n",
    " [ 1.10603018-1.52326858e+00j, -0.28967674+1.76124199e-01j],\n",
    " [ 1.30824927+2.71373697e-01j,  0.48945368-6.79578908e-01j],\n",
    " [-1.02900018+1.45773250e+00j, -0.0674089 +6.89698064e-01j],\n",
    " [ 0.18257806-1.31321964e+00j, -0.11576523-3.23022267e-01j],\n",
    " [ 1.58047119+1.04833512e+00j,  0.07952518+1.50635897e-01j],\n",
    " [ 1.01423439-1.50257983e+00j,  0.46529575-2.26337829e-01j],\n",
    " [-0.42480971-3.49892668e-01j, -0.30672725-1.18883560e-01j],\n",
    " [ 1.03955124-1.55138103e+00j,  0.23254546-1.38694212e-01j],\n",
    " [-1.49743323-1.06205861e+00j, -0.16771164-3.16520096e-01j],\n",
    " [-0.46856356-3.54025416e-01j, -0.3973579 +4.44297790e-02j],\n",
    " [ 1.35689016+1.83580815e-01j,  0.27104143-6.61877972e-02j],\n",
    " [-0.63258635-1.20440973e+00j, -0.01050379-3.07931313e-02j],\n",
    " [ 1.24269807-6.52671892e-01j, -0.14084624-2.82161383e-01j],\n",
    " [-0.54027418-3.82956684e-01j,  0.37467908-3.74822378e-02j],\n",
    " [-1.47353323-1.07912038e+00j, -0.56170325-3.80165877e-02j],\n",
    " [-0.54426269-3.72513503e-01j,  0.04006859-1.70839871e-01j],\n",
    " [-1.19577121+6.51486323e-01j, -0.32825071+1.55608461e-01j],\n",
    " [-0.4007257 +4.93670146e-01j,  0.077162  +9.55855061e-02j],\n",
    " [-0.3045899 +4.95747177e-01j, -0.22736721+2.32488967e-01j],\n",
    " [ 1.04657877-1.54056260e+00j,  0.24590774-1.35182233e-01j],\n",
    " [-0.62752268-1.21811080e+00j, -0.13211342-3.30344308e-01j],\n",
    " [-1.53833441-1.10877849e+00j, -0.12297664-6.88482547e-02j],\n",
    " [-1.30842872-1.96725117e-01j, -0.56619321-4.90123036e-02j]], dtype = complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f52bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cma_error_dualpol(E_out, radii=[np.sqrt(0.2), np.sqrt(1), np.sqrt(1.8)]):\n",
    "\n",
    "    xpol = E_out[:, 0]\n",
    "    ypol = E_out[:, 1]\n",
    "\n",
    "    e_total = 0.0\n",
    "\n",
    "    for x_cap, y_cap in zip(xpol, ypol):\n",
    "        R_x = min(radii, key=lambda r: abs(np.abs(x_cap) - r))\n",
    "        R_y = min(radii, key=lambda r: abs(np.abs(y_cap) - r))\n",
    "\n",
    "        e_x = (np.abs(x_cap)**2 - R_x**2)**2\n",
    "        e_y = (np.abs(y_cap)**2 - R_y**2)**2\n",
    "        e_total += e_x + e_y\n",
    "\n",
    "    return e_total / len(xpol)\n",
    "\n",
    "def compute_reward(E_out):\n",
    "    return -cma_error_dualpol(E_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_coeffs, hidden_dim=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(2 * num_coeffs, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 4 * num_coeffs * 2)  # 4 channel * 3 coeffs per channel * real and imag parts for each coeff * inc/dec each\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(states),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(next_states)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, num_coeffs, delta=0.01, gamma=0.99, lr=0.005):\n",
    "        self.num_coeffs = num_coeffs\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.q_net = QNetwork(num_coeffs)\n",
    "        self.target_net = QNetwork(num_coeffs)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 4 * 2 * self.num_coeffs - 1)\n",
    "        with torch.no_grad():\n",
    "            q_vals = self.q_net(torch.FloatTensor(state))\n",
    "            return torch.argmax(q_vals).item()\n",
    "\n",
    "    def update(self, batch_size=64):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "        states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)\n",
    "        q_vals = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_q_vals = self.target_net(next_states).max(1)[0]\n",
    "            targets = rewards + self.gamma * next_q_vals\n",
    "        loss = self.loss_fn(q_vals, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def soft_update(self, tau=0.01):\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(E_in, pxx, pxy, pyx, pyy):\n",
    "    x_in, y_in = E_in[:, 0], E_in[:, 1]\n",
    "    x_out = np.convolve(x_in, pxx, mode='same') + np.convolve(y_in, pxy, mode='same')\n",
    "    y_out = np.convolve(x_in, pyx, mode='same') + np.convolve(y_in, pyy, mode='same')\n",
    "    return np.column_stack((x_out, y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, epsilon=0.975, last_loss=10.586462, reward=-46.169883\n",
      "Episode 10, epsilon=0.951, last_loss=21.039953, reward=-32.233525\n",
      "Episode 15, epsilon=0.928, last_loss=37.300518, reward=-38.272852\n",
      "Episode 20, epsilon=0.905, last_loss=59.814518, reward=-42.477450\n",
      "Episode 25, epsilon=0.882, last_loss=44.128712, reward=-37.264636\n",
      "Episode 30, epsilon=0.860, last_loss=56.945923, reward=-31.282833\n",
      "Episode 35, epsilon=0.839, last_loss=59.044098, reward=-43.473316\n",
      "Episode 40, epsilon=0.818, last_loss=102.001694, reward=-37.105929\n",
      "Episode 45, epsilon=0.798, last_loss=53.052933, reward=-34.786723\n",
      "Episode 50, epsilon=0.778, last_loss=42.708038, reward=-26.180894\n",
      "Episode 55, epsilon=0.759, last_loss=104.253487, reward=-37.239535\n",
      "Episode 60, epsilon=0.740, last_loss=53.682137, reward=-29.190973\n",
      "Episode 65, epsilon=0.722, last_loss=67.921898, reward=-30.152138\n",
      "Episode 70, epsilon=0.704, last_loss=54.173428, reward=-37.472959\n",
      "Episode 75, epsilon=0.687, last_loss=108.958008, reward=-29.429242\n",
      "Episode 80, epsilon=0.670, last_loss=133.379929, reward=-28.239064\n",
      "Episode 85, epsilon=0.653, last_loss=106.552322, reward=-33.552968\n",
      "Episode 90, epsilon=0.637, last_loss=101.579689, reward=-33.617623\n",
      "Episode 95, epsilon=0.621, last_loss=34.977039, reward=-35.523807\n",
      "Episode 100, epsilon=0.606, last_loss=58.616241, reward=-28.276278\n",
      "Episode 105, epsilon=0.591, last_loss=159.948898, reward=-36.146027\n",
      "Episode 110, epsilon=0.576, last_loss=44.648872, reward=-43.873247\n",
      "Episode 115, epsilon=0.562, last_loss=48.475677, reward=-38.775742\n",
      "Episode 120, epsilon=0.548, last_loss=51.284313, reward=-29.538731\n",
      "Episode 125, epsilon=0.534, last_loss=90.925941, reward=-31.038157\n",
      "Episode 130, epsilon=0.521, last_loss=130.080887, reward=-41.534588\n",
      "Episode 135, epsilon=0.508, last_loss=130.827148, reward=-24.430749\n",
      "Episode 140, epsilon=0.496, last_loss=95.464706, reward=-35.665032\n",
      "Episode 145, epsilon=0.483, last_loss=93.372009, reward=-25.352326\n",
      "Episode 150, epsilon=0.471, last_loss=43.577759, reward=-28.285297\n",
      "Episode 155, epsilon=0.460, last_loss=18.762806, reward=-15.323167\n",
      "Episode 160, epsilon=0.448, last_loss=46.032707, reward=-24.997732\n",
      "Episode 165, epsilon=0.437, last_loss=103.746376, reward=-28.673357\n",
      "Episode 170, epsilon=0.427, last_loss=42.033215, reward=-37.846404\n",
      "Episode 175, epsilon=0.416, last_loss=31.777580, reward=-21.529708\n",
      "Episode 180, epsilon=0.406, last_loss=160.505920, reward=-31.815740\n",
      "Episode 185, epsilon=0.396, last_loss=69.929626, reward=-30.012034\n",
      "Episode 190, epsilon=0.386, last_loss=53.512859, reward=-26.805163\n",
      "Episode 195, epsilon=0.376, last_loss=34.489662, reward=-14.912532\n",
      "Episode 200, epsilon=0.367, last_loss=40.001801, reward=-12.464660\n",
      "Episode 205, epsilon=0.358, last_loss=57.481930, reward=-11.235411\n",
      "Episode 210, epsilon=0.349, last_loss=26.369011, reward=-15.155940\n",
      "Episode 215, epsilon=0.340, last_loss=61.492035, reward=-16.517219\n",
      "Episode 220, epsilon=0.332, last_loss=24.645222, reward=-13.115517\n",
      "Episode 225, epsilon=0.324, last_loss=33.298946, reward=-13.330148\n",
      "Episode 230, epsilon=0.316, last_loss=18.619827, reward=-19.842755\n",
      "Episode 235, epsilon=0.308, last_loss=48.288483, reward=-14.174077\n",
      "Episode 240, epsilon=0.300, last_loss=14.713849, reward=-12.492150\n",
      "Episode 245, epsilon=0.293, last_loss=16.710375, reward=-11.788324\n",
      "Episode 250, epsilon=0.286, last_loss=20.239155, reward=-14.998550\n",
      "Episode 255, epsilon=0.279, last_loss=69.380951, reward=-13.638105\n",
      "Episode 260, epsilon=0.272, last_loss=23.610798, reward=-14.423406\n",
      "Episode 265, epsilon=0.265, last_loss=17.676292, reward=-10.818435\n",
      "Episode 270, epsilon=0.258, last_loss=18.498884, reward=-11.770518\n",
      "Episode 275, epsilon=0.252, last_loss=11.960594, reward=-11.269404\n",
      "Episode 280, epsilon=0.246, last_loss=15.132071, reward=-11.726918\n",
      "Episode 285, epsilon=0.240, last_loss=12.200523, reward=-14.606540\n",
      "Episode 290, epsilon=0.234, last_loss=7.174139, reward=-10.957537\n",
      "Episode 295, epsilon=0.228, last_loss=5.888417, reward=-8.782885\n",
      "Episode 300, epsilon=0.222, last_loss=8.257861, reward=-9.677800\n",
      "Episode 305, epsilon=0.217, last_loss=44.738213, reward=-16.364390\n",
      "Episode 310, epsilon=0.211, last_loss=10.976668, reward=-11.257387\n",
      "Episode 315, epsilon=0.206, last_loss=5.170050, reward=-11.970762\n",
      "Episode 320, epsilon=0.201, last_loss=17.663286, reward=-13.560980\n",
      "Episode 325, epsilon=0.196, last_loss=4.838429, reward=-10.807413\n",
      "Episode 330, epsilon=0.191, last_loss=23.462854, reward=-20.985644\n",
      "Episode 335, epsilon=0.187, last_loss=10.748816, reward=-9.145894\n",
      "Episode 340, epsilon=0.182, last_loss=17.668043, reward=-20.049883\n",
      "Episode 345, epsilon=0.177, last_loss=21.264406, reward=-19.132111\n",
      "Episode 350, epsilon=0.173, last_loss=17.229691, reward=-15.243022\n",
      "Episode 355, epsilon=0.169, last_loss=14.538295, reward=-23.300583\n",
      "Episode 360, epsilon=0.165, last_loss=5.658914, reward=-12.378025\n",
      "Episode 365, epsilon=0.160, last_loss=16.372883, reward=-10.211445\n",
      "Episode 370, epsilon=0.157, last_loss=5.216209, reward=-12.112898\n",
      "Episode 375, epsilon=0.153, last_loss=16.798248, reward=-15.969825\n",
      "Episode 380, epsilon=0.149, last_loss=5.112647, reward=-9.248949\n",
      "Episode 385, epsilon=0.145, last_loss=10.525665, reward=-10.003314\n",
      "Episode 390, epsilon=0.142, last_loss=6.284495, reward=-11.335464\n",
      "Episode 395, epsilon=0.138, last_loss=11.238813, reward=-7.850216\n",
      "Episode 400, epsilon=0.135, last_loss=15.147718, reward=-11.514879\n",
      "\n",
      "Training complete\n",
      "Final filters:\n",
      "pxx =  [(0.19278588742874891+0.1565967028251923j), (0.6384388346473834-0.22030788976716492j), (0.11833568346028862+0.5354418804765676j)]\n",
      "pxy =  [(0.6018276414993511-0.39189081610647436j), (0.5600745006595894+0.6758716110364135j), (0.09195882344299255+0.8283914307009889j)]\n",
      "pyx =  [(0.4490470782749889+0.5629232400353172j), (0.5395070851702994-0.6948455559193323j), (0.3442021079765213-0.011228889783023114j)]\n",
      "pyy =  [(0.24222171226176356-0.6856413597730969j), (0.15826636212898026+0.8327775913293115j), (0.8121226200898618+0.24991332750261178j)]\n"
     ]
    }
   ],
   "source": [
    "num_taps = 3\n",
    "num_coeffs = num_taps * 4   # 4 channels * 3 taps = 12 complex coeffs\n",
    "num_episodes = 400\n",
    "steps_per_episode = 100\n",
    "delta = 0.02\n",
    "\n",
    "agent = DQNAgent(num_coeffs=num_coeffs, delta=delta)\n",
    "epsilon, epsilon_min, epsilon_decay = 1.0, 0.05, 0.995\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # initialize filter weights\n",
    "    pxx =  [(0.19278588742874891+0.13659670282519232j), (0.5184388346473833-0.5003078897671651j), (0.718335683460289+0.8954418804765679j)]\n",
    "    pxy =  [(0.6218276414993511-0.39189081610647436j), (0.5600745006595894+0.6758716110364135j), (0.09195882344299255+0.8283914307009889j)]\n",
    "    pyx =  [(0.4490470782749889+0.8029232400353175j), (0.4795070851702994-0.6948455559193323j), (0.32420210797652127+0.04877111021697689j)]\n",
    "    pyy =  [(0.22222171226176357-0.6656413597730969j), (0.15826636212898026+0.8327775913293115j), (0.7921226200898618+0.24991332750261175j)]\n",
    "\n",
    "    # ideal coefficients:\n",
    "    # pxx = [0.00337932-0.01182812j, 0.76123428-0.00134919j, 0.01344666+0.03096398j]\n",
    "    # pxy = [-0.01832318-0.01279765j, -0.03853096-0.00590369j,  0.02429311+0.00777087j]\n",
    "    # pyx = [-0.00104689+0.00372763j, -0.00066557+0.0018656j,   0.00047741+0.00161773j]\n",
    "    # pyy = [-3.46045693e-04+6.31402837e-04j,  9.99992221e-01+1.05758208e-06j,   -5.75912688e-05+3.69694991e-04j]\n",
    "\n",
    "    # pxx = np.zeros(num_taps, dtype=complex); pxx[num_taps//2] = 1\n",
    "    # pxy = np.zeros(num_taps, dtype=complex)\n",
    "    # pyx = np.zeros(num_taps, dtype=complex)\n",
    "    # pyy = np.zeros(num_taps, dtype=complex); pyy[num_taps//2] = 1\n",
    "\n",
    "    # state = real and imag parts of all coefficients\n",
    "    state = np.concatenate([\n",
    "        np.real(np.concatenate([pxx, pxy, pyx, pyy])),\n",
    "        np.imag(np.concatenate([pxx, pxy, pyx, pyy]))\n",
    "    ])\n",
    "\n",
    "    for t in range(steps_per_episode):\n",
    "        action = agent.select_action(state, epsilon)\n",
    "\n",
    "        filt_idx = action // (num_taps * 8)\n",
    "        tap_idx = (action // 4) % num_taps\n",
    "        part = (action // 2) % 2  # 0=real, 1=imag\n",
    "        direction = 1 if action % 2 == 0 else -1\n",
    "\n",
    "        filters = [pxx, pxy, pyx, pyy]\n",
    "        filt = filters[filt_idx]\n",
    "\n",
    "        # update chosen coefficient\n",
    "        if part == 0:\n",
    "            filt[tap_idx] += direction * delta\n",
    "        else:\n",
    "            filt[tap_idx] += 1j * direction * delta\n",
    "        filters[filt_idx] = filt\n",
    "\n",
    "        E_out = apply_filters(E_in_matrix, *filters)\n",
    "        reward = compute_reward(E_out)\n",
    "\n",
    "        next_state = np.concatenate([\n",
    "            np.real(np.concatenate(filters)),\n",
    "            np.imag(np.concatenate(filters))\n",
    "        ])\n",
    "\n",
    "        agent.replay_buffer.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        loss = agent.update()\n",
    "        agent.soft_update()\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if (episode + 1) % 5 == 0:\n",
    "        print(f\"Episode {episode+1}, epsilon={epsilon:.3f}, last_loss={loss:.6f}, reward={reward:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete\")\n",
    "print(\"Final filters:\")\n",
    "print(\"pxx = \", pxx)\n",
    "print(\"pxy = \", pxy)\n",
    "print(\"pyx = \", pyx)\n",
    "print(\"pyy = \", pyy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
